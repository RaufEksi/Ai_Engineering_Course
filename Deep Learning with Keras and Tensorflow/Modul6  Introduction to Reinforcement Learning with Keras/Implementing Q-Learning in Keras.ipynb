{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyN5283TZq4lNyCzmI7U7EcT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iAHRxB05lZwS","executionInfo":{"status":"ok","timestamp":1760292368215,"user_tz":-180,"elapsed":6329,"user":{"displayName":"rauf ekşi","userId":"07760405362537894175"}},"outputId":"04cd73d2-e2c2-4e84-966c-479ee9af9378"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym in /usr/local/lib/python3.12/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from gym) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gym) (3.1.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from gym) (0.1.0)\n"]}],"source":["!pip install gym\n"]},{"cell_type":"code","source":["import sys\n","sys.setrecursionlimit(1500)\n"],"metadata":{"id":"fBbzEU3CmLKq","executionInfo":{"status":"ok","timestamp":1760292368223,"user_tz":-180,"elapsed":2,"user":{"displayName":"rauf ekşi","userId":"07760405362537894175"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import gym\n","import numpy as np\n","\n","env = gym.make('CartPole-v1')\n","\n","np.random.seed(42)\n","env.action_space.seed(42)\n","env.observation_space.seed(42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cc-zDROCmdfo","executionInfo":{"status":"ok","timestamp":1760292407671,"user_tz":-180,"elapsed":209,"user":{"displayName":"rauf ekşi","userId":"07760405362537894175"}},"outputId":"ffe1ae16-729e-4ed7-bd9c-ca49cbc29acd"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n","Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n","See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n","/usr/local/lib/python3.12/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.12/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n","  return datetime.utcnow().replace(tzinfo=utc)\n"]},{"output_type":"execute_result","data":{"text/plain":["[42]"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Input  # Import Input layer\n","from tensorflow.keras.optimizers import Adam\n","import gym  # Ensure the environment library is available\n","\n","# Define the model building function\n","def build_model(state_size, action_size):\n","    model = Sequential()\n","    model.add(Input(shape=(state_size,)))  # Use Input layer to specify the input shape\n","    model.add(Dense(24, activation='relu'))\n","    model.add(Dense(24, activation='relu'))\n","    model.add(Dense(action_size, activation='linear'))\n","    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n","    return model\n","\n","# Create the environment and set up the model\n","env = gym.make('CartPole-v1')\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n\n","model = build_model(state_size, action_size)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WNU8XO1amohM","executionInfo":{"status":"ok","timestamp":1760292447992,"user_tz":-180,"elapsed":6026,"user":{"displayName":"rauf ekşi","userId":"07760405362537894175"}},"outputId":"91973622-7722-4ed3-af0a-9fa966f47eda"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.12/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n","  return datetime.utcnow().replace(tzinfo=utc)\n"]}]},{"cell_type":"code","source":["import random\n","import numpy as np\n","from collections import deque\n","import tensorflow as tf\n","\n","# Define epsilon and epsilon_decay\n","epsilon = 1.0  # Starting with a high exploration rate\n","epsilon_min = 0.01  # Minimum exploration rate\n","epsilon_decay = 0.99  # Faster decay rate for epsilon after each episode\n","\n","# Replay memory\n","memory = deque(maxlen=2000)\n","\n","def remember(state, action, reward, next_state, done):\n","    \"\"\"Store experience in memory.\"\"\"\n","    memory.append((state, action, reward, next_state, done))\n","\n","def replay(batch_size=64):  # Increased batch size\n","    \"\"\"Train the model using a random sample of experiences from memory.\"\"\"\n","    if len(memory) < batch_size:\n","        return  # Skip replay if there's not enough experience\n","\n","    minibatch = random.sample(memory, batch_size)  # Sample a random batch from memory\n","\n","    # Extract information for batch processing\n","    states = np.vstack([x[0] for x in minibatch])\n","    actions = np.array([x[1] for x in minibatch])\n","    rewards = np.array([x[2] for x in minibatch])\n","    next_states = np.vstack([x[3] for x in minibatch])\n","    dones = np.array([x[4] for x in minibatch])\n","\n","    # Predict Q-values for the next states in batch\n","    q_next = model.predict(next_states)\n","    # Predict Q-values for the current states in batch\n","    q_target = model.predict(states)\n","\n","    # Vectorized update of target values\n","    for i in range(batch_size):\n","        target = rewards[i]\n","        if not dones[i]:\n","            target += 0.95 * np.amax(q_next[i])  # Update Q value with the discounted future reward\n","        q_target[i][actions[i]] = target  # Update only the taken action's Q value\n","\n","    # Train the model with the updated targets in batch\n","    model.fit(states, q_target, epochs=1, verbose=0)  # Train in batch mode\n","\n","    # Reduce exploration rate (epsilon) after each training step\n","    global epsilon\n","    if epsilon > epsilon_min:\n","        epsilon *= epsilon_decay\n","\n","def act(state):\n","    \"\"\"Choose an action based on the current state and exploration rate.\"\"\"\n","    if np.random.rand() <= epsilon:\n","        return random.randrange(action_size)  # Explore: choose a random action\n","    act_values = model.predict(state)  # Exploit: predict action based on the state\n","    return np.argmax(act_values[0])  # Return the action with the highest Q-value\n","\n","# Define the number of episodes you want to train the model for\n","episodes = 10  # You can set this to any number you prefer\n","train_frequency = 5  # Train the model every 5 steps\n","\n","for e in range(episodes):\n","    state, info = env.reset()\n","    state = np.reshape(state, [1, state_size])\n","    for time in range(200):  # Limit to 200 time steps per episode\n","        action = act(state)  # Choose action for the current state\n","        next_state_raw, reward, terminated, truncated, info = env.step(action)\n","        done = terminated or truncated\n","        reward = reward if not done else -10\n","        next_state = np.reshape(next_state_raw, [1, state_size])\n","        remember(state, action, reward, next_state, done)  # Store experience with reshaped states\n","        state = next_state # Update the current state to the next state\n","\n","        if done:\n","            print(f\"episode: {e+1}/{episodes}, score: {time}, e: {epsilon:.2}\")\n","            break\n","\n","        # Train the model every 'train_frequency' steps\n","        if time % train_frequency == 0:\n","            replay(batch_size=64)  # Call replay with larger batch size for efficiency\n","\n","env.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"3CSCntaHmw8c","executionInfo":{"status":"error","timestamp":1760293019315,"user_tz":-180,"elapsed":3,"user":{"displayName":"rauf ekşi","userId":"07760405362537894175"}},"outputId":"a117d524-d300-4a57-9010-52e9b348136e"},"execution_count":9,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"too many values to unpack (expected 2)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-795367912.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Unpack the tuple returned by env.reset()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Choose initial action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]},{"cell_type":"code","source":[],"metadata":{"id":"rfXbG8iqoX0q"},"execution_count":null,"outputs":[]}]}